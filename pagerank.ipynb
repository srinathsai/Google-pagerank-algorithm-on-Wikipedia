{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Project by Srinath Sai Tripuraneni (A20338902)."
      ],
      "metadata": {
        "id": "CgwdRO_zj9hl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Initial process of making M into file and r_old into file"
      ],
      "metadata": {
        "id": "b2CNA__TDhtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #importing drive folder."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNa7DGSthBlg",
        "outputId": "7d5acc2e-16eb-4efc-fa42-656fa03e9458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note :- Here I have uploaded the given 2 folders in my drive , and took that path ,for execution you need to specify your folder path , this is the initial process of constructing M file which takes some time and this time is not considered in execution time."
      ],
      "metadata": {
        "id": "Zfa-C9GTK6Nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#folder = ''\n",
        "import os\n",
        "import re\n",
        "#articles_list = []\n",
        "M_matrix=open('M.txt','w') #opening new empty M file.\n",
        "#r_old_initial=open('r_old','w')\n",
        "for root, dirs, files in os.walk(\"/content/drive/MyDrive/Dataset\"): #iteraing over directory\n",
        "    for file in files: #iterating per file,because here every file is an article instead of combining all files to a single file thaught to iterate over file by file which is article by article for memory efficiency.\n",
        "        if file.endswith('.txt'): #though all are text files this is just an block to perform cleaning operation.\n",
        "            with open(os.path.join(root, file), 'r') as f: #opening every article\n",
        "                text = f.read() #reading every article matter in string format.\n",
        "                y=text.split('</title') #as source of page is between <title> and </title>, just splitted on </title> which makes entire article matter into 2 space array in which 1st index will have <title> source \n",
        "                #next index will have rest of article matter excluding </title>\n",
        "                z=y[0].split('<title>') #now again spliting first index of list on <title> which will give source in z[1] place.\n",
        "                each_row=() #taking tuple which will stores first element as source, second element is degree and third element is list of destinations.\n",
        "                m=[m1.start() for m1 in re.finditer('\\[\\[', y[1])] # as our destinations are in between [[ ]], first finding indices of [[ and storing them in list m.\n",
        "                n=[n1.start() for n1 in re.finditer('\\]\\]', y[1])] # using re.finditer finding indices of only ]] and storing in list n.\n",
        "                destinations=[] #list of destinations to extract strings between indices in list m and list n.\n",
        "                if((len(m)>0 and len(n)>0) and (len(m)==len(n))): #this is important step as some articles may contain only ]] or may contain only [[ which are not required.\n",
        "                  for i in range(0,len(m)):# after filtering we will proceed as [[ ]] these lists length should be equal.\n",
        "                    if(y[1][m[i]+2:n[i]] not in destinations): #now extracting strings between [[ ]] by substring from main string of matter article, using indices stored in m and n.\n",
        "                      destinations.append(y[1][m[i]+2:n[i]]) # appending destinations extracted in [[ ]].\n",
        "                each_row=(z[1],len(destinations),destinations) # appending in tuple as source, degree (len of list of destinations), destinations list.\n",
        "                if(each_row[1]>0): # eleminating the sources which are having zero degrees.\n",
        "                  M_matrix.write(str(each_row[0]) + '||#' + str(each_row[1]) + '||#' +  str(each_row[2])) # writing into M file.\n",
        "                  M_matrix.write(\"\\n\") # for not concatenating with next line.\n",
        "                  #r_old_initial.write(str(each_row[0]) + ':-' + )\n",
        "M_matrix.close()\n",
        "                  \n",
        "#FINALLY AFTER THIS ITERATION MY M.txt WILL HAVE LINES OF SOURCES IN THE FORMAT 'SOURCE'||#'DEGREE'||#'DESTINATIONS' . I PURPOSEDLY WRITTEN INTO FILE LIKE THIS SO THAT IN ALGORITHM I CAN\n",
        "#DIRECTLY SPLIT ON '||#'.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BqBs0isupVPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(r\"M.txt\", 'r') as fp:\n",
        "  N = len(fp.readlines()) #getting N value for the formula.\n",
        "  print('Total Number of Page sources in M without zero degree:', N)\n"
      ],
      "metadata": {
        "id": "FogBxqaNY1s1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c95a215-9202-4100-ca64-887a519afe8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Page sources in M without zero degree: 38302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "beta=0.8 #given beta value.\n",
        "r_new={} #storing r_new hashmap which is allowed and why hashmap because for specific destination updation must be done right so keys will be graph nodes and values will be page ranks"
      ],
      "metadata": {
        "id": "grmaaVP0lLEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CREATING INTIAL r_old FILE WITH EVERY SOURCE AND  1/N PAGE RANK VALUE.\n",
        "r_old_initial=open('r_old','w') #intialing r_old empty file for writing inital r old.\n",
        "with open('M.txt') as f:\n",
        "    for line in f:\n",
        "        line_cache=line.split('||#') #from M file for every line, splitting on ||# which will give us three spaced array in which first space is source , second space will be degree and third space will be list of destinations.\n",
        "        r_old_initial.write(str(line_cache[0]) + ':-' + str(1/N)) #for every source writing as source :- 1/N \n",
        "        r_old_initial.write(\"\\n\") # for not concatenating next line.\n",
        "        #r_new[line_cache[0]]=((1-beta)/N)\n",
        "r_old_initial.close()\n",
        "\n",
        "#SO FINALLY r_old FILE WILL HAVE ALL NODES AS NODE :- and 1/N VALUE RESPECTIVELY."
      ],
      "metadata": {
        "id": "H7r0-ObQmcj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('r_old') as f:\n",
        "  for line in f:           \n",
        "    rank_cache=line.split(':-')   # now from r_old file initializing r_new for every line I will split based on :- to get nodes.\n",
        "    r_new[rank_cache[0]]=((1-beta)/N) #storing in hashmap with respective node as key and (1-beta)/N"
      ],
      "metadata": {
        "id": "lynhZLZv9Ym7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Algorithm implementation"
      ],
      "metadata": {
        "id": "aP5-BSGlDxlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import literal_eval #necessary package for extracting destinations in algorithm."
      ],
      "metadata": {
        "id": "_QD0JNtfJh3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time #time module has been exported.\n"
      ],
      "metadata": {
        "id": "10S4wF0Tx7_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time() #time of execution starts here.\n",
        "for i in range(1,16): # 15 iterations of entire algorithm.\n",
        "  #sum=0\n",
        "  #here r_new have all sources right, so for every page in first line of algorithm means iterating over hashmap which is only in memory.\n",
        "  for x in r_new: # for every source according to algorithm we should read M file for destinations, degree and read r_old file for respective value;\n",
        "    destinations=[]\n",
        "    degree=0 #intializing all required variables for every source. before opening M file and r_old File\n",
        "    r_old_value=0.000000\n",
        "    with open(r\"M.txt\",'r') as f: #Here M file has been opened.\n",
        "      for line_number,line in enumerate(f): #iterating line by line of file instead of storing all mater for memory efficency.\n",
        "        line_cache=line.split('||#') #every line will be splitted on ||# for every iteration.\n",
        "        if(x==line_cache[0]): # if the page source x in r_new is same as source in M\n",
        "          degree=int(line_cache[1]) # taking degree of that source\n",
        "          destinations=literal_eval(line_cache[2]) #literal_eval is the python package which will extract without changing datastructure format from string. So we get destinations list.\n",
        "          break # when we get destinations and degree for one page breaking the loop of scanning of file for time efficiency.\n",
        "    with open(r\"r_old\",'r') as f: #scanning of r_old file for respective r_old value.\n",
        "      for line_number,line in enumerate(f): #reading as for each line instead of storing entire matter for memory efficiency.\n",
        "        r_old_cache=line.split(':-') #splitting on :-\n",
        "        if(r_old_cache[0]==x): #again if we get any source equals to source in r_old file \n",
        "          m = re.findall(r'\\d+\\.\\d+', r_old_cache[1]) #this will give you digits in value from r_old matching line and from second element after splitting on ':-'\n",
        "          r_old_value=float(m[0]) #taking into float of respective value of matched source.\n",
        "          break #breaking the scan of file after getting respective page rank value for time efficiency.\n",
        "   \n",
        "    for destination in destinations:\n",
        "      if(destination in r_new):\n",
        "        for j in range(1,degree+1):                            #last part of algorithm after getting every page's old rank value, degree, destinations.\n",
        "          r_new[destination]=beta*(r_old_value/degree) +(1-beta)/N #appending new ranks with this formula.\n",
        "\n",
        "    sum=0\n",
        "    for x in r_new:\n",
        "      sum+=r_new[x] #taking sum of all pageranks after updating.\n",
        "    for x in r_new:\n",
        "      r_new[x]+=(1-sum)/N #again updating for all pageranks and this ensures less leaking pages.\n",
        "    r_new1=open('r_old','w')\n",
        "    for y in r_new:\n",
        "      r_new1.write(str(y) + ':-' + str(r_new[y])) # this final step for every iteration in which we overwrite r_new values to r_old in r_old file. . This is exacltly doing r_new to r_old after every iteration.\n",
        "      r_new1.write(\"\\n\") #at the end of each iteration r_new is similary written as same format as r_old.\n",
        "    r_new1.close()\n",
        "\n",
        "end_time=time.time()\n",
        "  #end time will be calculated after each iteration.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wXgPU_BDDwe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('execution time is :-' + str(end_time-start_time)) #execution time of my implementation in seconds."
      ],
      "metadata": {
        "id": "Q-F2NwExyHOo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35a38fa6-bae5-413a-aec2-3285160f6744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "execution time is :-63509.9616894722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Validity of my implementation."
      ],
      "metadata": {
        "id": "6T-F76mosDZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generally all page ranks sum should be 1 ideally, but in our graph there are some nodes which can leak and reduce this rank but very slightly."
      ],
      "metadata": {
        "id": "7TGJtR9CdfWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum=0;\n",
        "for x in r_new: #summing of all page ranks in stored hashmap\n",
        "  sum+=r_new[x]\n",
        "print(sum) # this sums show that my algorithm implementation is valid as because after 15 iterations it reduced from 1 by very very less significant amount by 1."
      ],
      "metadata": {
        "id": "CzvUgeZiYSXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b033a4f-bc44-4a60-98c6-1039366a1b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9997553395888826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Output"
      ],
      "metadata": {
        "id": "L4a0JVJ9vNzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict #package for arranging values based on order.\n",
        "descending_page_ranks = OrderedDict(sorted(r_new.items(), key=lambda x: x[1], reverse=True)) #from r_newsorting based on decending page ranks.\n",
        "output=[] # output list for display.\n",
        "for x in descending_page_ranks:\n",
        "  output.append((x,descending_page_ranks[x]/1000)) # here every page value is very very less which can have something as 3.something *e^-5 something. but printing as string format is allowed only taking number not e^-5 thing ,\n",
        "  #since all ranks sum must be 0.999755339588826 from above to not look awkward divided by 1000 but I checked every value of the ranks and observed to be *e^-5 something which justifies my validity algorithm.\n",
        "for i in range(0,25): # printing top 25 pages with their ranks.\n",
        "  print(str(output[i][0]) + \":\" , output[i][1])"
      ],
      "metadata": {
        "id": "KlvpfRWbvNG8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93f4385d-1648-4b49-e79c-ed42ebf7ce91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "James Edward Keeler: 0.007904241858745437\n",
            "Duck: 0.007831784025668052\n",
            "Category:Social sciences: 0.007536642960715786\n",
            "Wallbach, Switzerland: 0.007485269058668212\n",
            "Fremantle, Western Australia: 0.007452869784595176\n",
            "Mary, Crown Princess of Denmark: 0.007337437524636131\n",
            "Sendero Luminoso: 0.00701641185389771\n",
            "Eastern long-beaked echidna: 0.006885054355985402\n",
            "Continental drift: 0.006702555004102609\n",
            "2004 Redfern riots: 0.006693185479334484\n",
            "Jester: 0.0066568727881050034\n",
            "Becker (TV series): 0.006522376225418934\n",
            "Sibling: 0.006378160679742978\n",
            "Virus: 0.0063163204500442955\n",
            "Western long-beaked echidna: 0.006241108373065668\n",
            "Salamander: 0.006188671101753893\n",
            "Psychokinesis: 0.006106585179142002\n",
            "May 1: 0.00591527877121967\n",
            "Tyndareus: 0.005779078786611759\n",
            "Christoph Willibald von Gluck: 0.005700219173290185\n",
            "Union for a Popular Movement: 0.005682415784185709\n",
            "Sanjo Station (Kyoto): 0.0056181284141373685\n",
            "Twin town: 0.005580782777799577\n",
            "Tour of Britain: 0.0055072167056761465\n",
            "Arachnid: 0.005479772816033096\n"
          ]
        }
      ]
    }
  ]
}